{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # for HTTPS requests to URLS\n",
    "import zipfile  # to un-zip any data files\n",
    "import pypdf  # to extract text from PDFs (is NOT that good)\n",
    "import sys, pathlib, fitz  # to extract text from PDFs\n",
    "import bs4  # HTLM parser to extract data from HTML pages\n",
    "import math\n",
    "from sympy import divisors\n",
    "import pandas  # to create and save CSV files\n",
    "import time  # sleep function to manage rate limits for Congress API\n",
    "import progress  # progress barfor loops (not used)\n",
    "from tqdm import tqdm  # progress bar for loops\n",
    "import pdf2docx  # good method to extract text (in correct order) from PDFs\n",
    "import docx2txt  # Method for converting a Docx file to a Txt file\n",
    "from pdfminer.high_level import extract_text  # YES, use this for text extraction from PDFs\n",
    "import torch  # PyTorch for creating machine learning models\n",
    "import datetime  # For creating different Supreme Court groups\n",
    "import re  # For regular expressions\n",
    "import string  # For string cleaning\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "from itertools import chain\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import matplotlib as plt\n",
    "import numpy\n",
    "\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "# import lzma\n",
    "# import torchtext  # For tokenizer\n",
    "# from torchtext.data import tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U.S. Supreme Court opinions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect a list of all Justices and their service dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date ranges for Supreme Courts with a unique set of justices\n",
    "url = \"https://www.supremecourt.gov/about/members_text.aspx\"\n",
    "r = requests.get(url=url)\n",
    "\n",
    "soup = bs4.BeautifulSoup(markup=r.content, features=\"html.parser\")\n",
    "justice_tables = soup.find_all(attrs={\"class\":\"table table-striped justicetable\"})\n",
    "\n",
    "soup = bs4.BeautifulSoup(markup=str(justice_tables[0]), features=\"html.parser\")\n",
    "chief_justices_data = list(map(lambda x: x.text.split(\"\\n\"), soup.find_all(\"tr\")[1:]))\n",
    "soup = bs4.BeautifulSoup(markup=str(justice_tables[1]), features=\"html.parser\")\n",
    "associate_justices_data = list(map(lambda x: x.text.split(\"\\n\"), soup.find_all(\"tr\")[1:]))\n",
    "all_justices_data = chief_justices_data + associate_justices_data\n",
    "\n",
    "def cleanNotes(s):\n",
    "    return s.replace(\"(a) \", \"\").replace(\"(b) \", \"\").replace(\"(c) \", \"\").replace(\"*\", \"\").strip()\n",
    "\n",
    "def toDate(s):\n",
    "    if s == \"\":\n",
    "        return False\n",
    "    # Two formatting mistakes on the Supreme Court's website makes this necessary (notification has been submitted to the Webmaster)\n",
    "    try:\n",
    "        return datetime.datetime.strptime(s, \"%B %d, %Y\").date()\n",
    "    except:\n",
    "        try:\n",
    "            return datetime.datetime.strptime(s, \"%B %d %Y\").date()\n",
    "        except:\n",
    "            return datetime.datetime.strptime(s, \"%B %d,%Y\").date()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of Chief Justice Supreme Courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "chief_justices = list(map(lambda x: [x[1], toDate(cleanNotes(x[4])), toDate(cleanNotes(x[5]))], chief_justices_data))\n",
    "\n",
    "chief_courts = list(map(lambda x: (x[0][:x[0].find(\",\")] + \" Court\", x[1], x[2]), chief_justices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of unique member-set Supreme Courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_justices = list(map(lambda x: [x[1], toDate(cleanNotes(x[4])), toDate(cleanNotes(x[5]))], all_justices_data))\n",
    "\n",
    "start_dates = list(map(lambda x: x[1], all_justices))\n",
    "end_dates = list(filter(lambda x: x, map(lambda x: x[2], all_justices)))\n",
    "start_dates.sort()\n",
    "beginning = start_dates[0]\n",
    "end_dates.sort(reverse=True)\n",
    "end = end_dates[0]\n",
    "\n",
    "time_period = []\n",
    "one_day = datetime.timedelta(days=1)\n",
    "curr = beginning\n",
    "\n",
    "for i in range((end - beginning).days + 1):\n",
    "    time_period.append((curr, []))\n",
    "    curr = curr + one_day\n",
    "\n",
    "for justice in all_justices:\n",
    "\n",
    "    # Finds the index in the time_period list where their service starts\n",
    "    start_index = (justice[1] - beginning).days\n",
    "    \n",
    "    # Finds the index in the time_period list where their service ends\n",
    "    if justice[2] == False: # if there is no end date\n",
    "        end_index = len(time_period) # make end index the end of the list\n",
    "    else: # if there is an end date\n",
    "        end_index = (justice[2] - beginning).days # find the date index in time_period list\n",
    "\n",
    "    for day_index in range(start_index, end_index):\n",
    "\n",
    "        time_period[day_index][1].append(justice[0])\n",
    "\n",
    "# Initialize pointer / trackers\n",
    "court_sets = []\n",
    "prev_court = set(time_period[0][1])\n",
    "prev_date = time_period[0][0]\n",
    "start_date = time_period[0][0]\n",
    "\n",
    "for day in time_period:\n",
    "    \n",
    "    curr_court = set(day[1])\n",
    "\n",
    "    if prev_court != curr_court:\n",
    "        \n",
    "        court_sets.append((prev_court, start_date, prev_date))  # add old court to list\n",
    "        start_date = day[0]  # set start date as current date\n",
    "        prev_date = day[0]  # set previous date as current date\n",
    "        prev_court = curr_court  # set previous court as current court\n",
    "\n",
    "    else:\n",
    "\n",
    "        prev_date = day[0]  # set previous date as current date\n",
    "\n",
    "# Add last court\n",
    "court_sets.append((prev_court, start_date, prev_date))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect decisions between 1937 and 1975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.govinfo.gov/bulkdata/SCD/1937/SCD-1937.zip\"\n",
    "r = requests.get(url=url)\n",
    "\n",
    "open(\"./data/raw-data/SCD/SCD-1937.zip\", \"xb\").write(r.content)\n",
    "with zipfile.ZipFile(file=\"./data/raw-data/SCD/SCD-1937.zip\", mode=\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data/raw-data/SCD\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect decisions between 1991 and 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for 1991 to 2009\n",
    "for volume in range(502,561):\n",
    "    url = f\"https://www.supremecourt.gov/opinions/boundvolumes/{volume}bv.pdf\"\n",
    "    r = requests.get(url=url)\n",
    "    open(f\"./data/raw-data/SCD/SCD-{volume}.pdf\", \"xb\").write(r.content)\n",
    "\n",
    "# Loop for 2009 to 2015\n",
    "for volume in range(561,578):\n",
    "    url = f\"https://www.supremecourt.gov/opinions/boundvolumes/{volume}BV.pdf\"\n",
    "    r = requests.get(url=url)\n",
    "    open(f\"./data/raw-data/SCD/SCD-{volume}.pdf\", \"xb\").write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2c151662024c0eac0107c60fe0fcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for volume in tqdm(range(502, 578)):\n",
    "\n",
    "    text = extract_text(f\"./data/raw-data/SCD/SCD-{volume}.pdf\")\n",
    "    data.append([volume, text])\n",
    "\n",
    "pandas.DataFrame(data=data, columns=[\"Volume\", \"Text\"]).to_csv(\"./data/raw-data/SCD.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U.S. Presidential Documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Presidential memorandas, determinations, executive orders, and proclamations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d927eff454c4faf8ed46a12eda710ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dictionary for all executive document-types and their associated url-bases\n",
    "executive = {\n",
    "    \"memorandas\": \"app-categories/presidential/memoranda\",\n",
    "    \"determinations\": \"app-attributes/determinations\",\n",
    "    \"executive-orders\": \"app-attributes/executive-orders\",\n",
    "    \"proclamations\": \"app-categories/written-presidential-orders/presidential/proclamations\"\n",
    "}\n",
    "\n",
    "for document_type, base in executive.items():\n",
    "\n",
    "    data = []\n",
    "    if document_type in [\"memorandas\", \"determinations\",  \"executive-orders\"]:\n",
    "        continue\n",
    "\n",
    "    # Get counts for page requests\n",
    "    url = f\"https://www.presidency.ucsb.edu/documents/{base}?items_per_page=5\"\n",
    "    r = requests.get(url=url)\n",
    "    soup = bs4.BeautifulSoup(markup=r.content, features=\"html.parser\")\n",
    "    count = soup.find(attrs={\"class\":\"tax-count\"}).text\n",
    "    total = int(count[count.index(\"of\")+3: count.index(\".\")])\n",
    "    MAX_DISPLAY = 500\n",
    "    display = max(map(lambda x: x if x<MAX_DISPLAY else 0,divisors(total)))\n",
    "    pageNum = int(total / display)\n",
    "\n",
    "    # Loop to gather all documents\n",
    "    for page in tqdm(range(21,pageNum)):\n",
    "        url = f\"https://www.presidency.ucsb.edu/documents/{base}?items_per_page={display}&page={page}\"\n",
    "        r = requests.get(url=url)\n",
    "        soup = bs4.BeautifulSoup(markup=r.content, features=\"html.parser\")\n",
    "        documentLinks = [child.a[\"href\"] for child in soup.find_all(attrs={\"class\":\"field-title\"})]\n",
    "\n",
    "        for href in documentLinks:\n",
    "            # get content\n",
    "            r = requests.get(url=f\"https://www.presidency.ucsb.edu{href}\")\n",
    "            soup = bs4.BeautifulSoup(markup=r.content, features=\"html.parser\")\n",
    "            content = \"\"\n",
    "            for child in soup.find(attrs={\"class\":\"field-docs-content\"}).children:\n",
    "                content += child.text\n",
    "            # Get person\n",
    "            soup = bs4.BeautifulSoup(markup=r.content, features=\"html.parser\")\n",
    "            person = \"\"\n",
    "            for child in soup.find(attrs={\"class\":\"diet-title\"}).children:\n",
    "                person += child.text\n",
    "\n",
    "            data.append([person, content])\n",
    "\n",
    "    pandas.DataFrame(data=data, columns=[\"person\", document_type]).to_csv(f\"./data/raw-data/{document_type}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Bill Summaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congress.gov now offers a beta [API](https://www.congress.gov/help/using-data-offsite) to request data available through their website."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://api.congress.gov/v3/summaries/\"\n",
    "url = \"https://api.congress.gov/v3/summaries?fromDateTime=1973-01-01T00:00:00Z&toDateTime=2023-01-02T00:00:00Z&sort=updateDate+asc&offset=0&limit=250&format=json\"\n",
    "start = \"1973-01-01T00:00:00Z\"\n",
    "end = \"2023-01-02T00:00:00Z\"\n",
    "params = {\n",
    "    \"api_key\": \"VdU8MBQu6ztVqNZ6WF6escCDRTXApyktwBdWqZli\",\n",
    "    \"limit\": 250,\n",
    "    \"fromDateTime\": start,\n",
    "    \"toDateTime\": end,\n",
    "    \"sort\": \"updateDate+asc\",\n",
    "}\n",
    "key_param = {\"api_key\": \"VdU8MBQu6ztVqNZ6WF6escCDRTXApyktwBdWqZli\",}\n",
    "offset = 0\n",
    "total = 398_395\n",
    "requestsNeeded = math.ceil(total/250)\n",
    "\n",
    "summaryData = []\n",
    "t = tqdm(total=total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 398395/398395 [3:59:52<00:00, 27.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "while (url != False):\n",
    "    \n",
    "    # params.update({\"offset\": offset}) # set offset to request new data\n",
    "    r = requests.get(url=url, params=key_param) # sent request to API\n",
    "    loopSummaries = r.json()[\"summaries\"]\n",
    "\n",
    "    # For the summary data from the request, map to collect the Chamber + Congress and the text of the summary\n",
    "    summaryData += map(\n",
    "        lambda data: [\n",
    "            data[\"bill\"][\"originChamber\"] + \" \" + str(data[\"bill\"][\"congress\"]),\n",
    "            data[\"text\"],\n",
    "        ], loopSummaries\n",
    "    )\n",
    "\n",
    "    url = r.json()[\"pagination\"].get(\"next\", False)\n",
    "    time.sleep(2)\n",
    "    t.update(n=len(loopSummaries))\n",
    "\n",
    "t.close()\n",
    "\n",
    "pandas.DataFrame(data=summaryData, columns=[\"Institution\", \"Text\"]).to_csv(\"data/congress3.csv\")  # save data to CSV file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "printable = set(string.printable)  # Get set of ASCII characters\n",
    "other = \"!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\"\n",
    "\n",
    "def cleanString(s):\n",
    "\n",
    "    s = \"\".join(filter(lambda x: x in printable, s))  # Remove all non-ASCII character from the string\n",
    "    return re.sub(\"\\s+\", \" \", s.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \")).strip()\n",
    "\n",
    "def bleachString(s):\n",
    "\n",
    "    onlyLetters = re.sub(r'[^a-z]', \" \", s.lower())  # remove numbers, and other non-letter characters\n",
    "    return re.sub(\"\\s+\", \" \", onlyLetters).strip()\n",
    "\n",
    "def totalClean(s):\n",
    "    return bleachString(cleanString(s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Congressional bill summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k4/5gf8l01j5kv4fl66gqqkkfv00000gn/T/ipykernel_18147/4264009358.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  lambda x: totalClean(bs4.BeautifulSoup(markup=x, features=\"html.parser\").text)\n"
     ]
    }
   ],
   "source": [
    "# Remove HTML artifacts, totally clean strings\n",
    "\n",
    "congress = pandas.read_csv(\"./data/raw-data/congress.csv\")\n",
    "\n",
    "congress[\"Text\"] = congress[\"Text\"].apply(\n",
    "    lambda x: totalClean(bs4.BeautifulSoup(markup=x, features=\"html.parser\").text)\n",
    "    )\n",
    "\n",
    "congress[\"Institution\"] = congress[\"Institution\"].apply(lambda x: x + \"th\")\n",
    "congress.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Supreme Court decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1937 through 1975 Supreme Court cases, there are 19 new lines between each case\n",
    "decision19 = 19 * \"\\n\"\n",
    "file = open(\"./data/raw-data/SCD/SCD-1937.txt\")\n",
    "decisions1937 = map(lambda x: cleanString(x), file.read().split(decision19))\n",
    "\n",
    "def getDate(s):\n",
    "\n",
    "    try:\n",
    "        match = re.search(r'(DECIDED|DECODED|DISMISSED|CONTINUED|ANNOUNCED|GRANTED)\\s[A-Z]+\\s[0-9]+,\\s[0-9]+', s)\n",
    "        date = match[0].split(\" \")\n",
    "        date = \" \".join(date[1:])\n",
    "        date = date.split(\" \")\n",
    "        date[0] = date[0][0] + date[0][1:].lower()\n",
    "        return \" \".join(date)\n",
    "    except:\n",
    "        print(s)\n",
    "\n",
    "def date2Cheif(date):\n",
    "\n",
    "    date = datetime.datetime.strptime(date, \"%B %d, %Y\").date()\n",
    "    \n",
    "    for chief in chief_courts:\n",
    "        \n",
    "        if date >= chief[1]:\n",
    "            val = chief[0]\n",
    "            if date <= chief[2]:\n",
    "                return chief[0]\n",
    "        else: # return previous Chief Justice if the start date of the current is before the decision\n",
    "            return val\n",
    "    \n",
    "\n",
    "\n",
    "decisions1937 = list(map(lambda x: [date2Cheif(getDate(x)), totalClean(x)], decisions1937))\n",
    "decisions1937 = pandas.DataFrame(data=decisions1937, columns=[\"Institution\", \"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions1991 = pandas.read_csv(\"./data/raw-data/SCD.csv\")\n",
    "\n",
    "decisions1991[\"Text\"] = decisions1991[\"Text\"].apply(lambda x: cleanString(x))\n",
    "\n",
    "def getChief(s):\n",
    "\n",
    "    try:\n",
    "        match = re.search(r'([A-Z]+|[A-Z]+,\\sJr\\.),\\sChief\\sJustice\\.', s)\n",
    "        return match[0].split(\",\")[0]\n",
    "    except:\n",
    "        print(\"Failed match\")\n",
    "\n",
    "decisions1991[\"Holder\"] = decisions1991[\"Text\"].apply(lambda x: getChief(x))\n",
    "decisions1991[\"Text\"] = decisions1991[\"Text\"].apply(lambda x: totalClean(x))\n",
    "decisions1991[\"Volume\"] = decisions1991[\"Holder\"].apply(lambda x: x[0] + x[1:].lower() + \" Court\")\n",
    "decisions1991.drop(columns=[\"Unnamed: 0\", \"Holder\"], inplace=True)\n",
    "decisions1991.rename(columns={\"Volume\": \"Institution\"}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Presidential documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "determinations = pandas.read_csv(\"./data/raw-data/determinations.csv\")\n",
    "executive_orders = pandas.read_csv(\"./data/raw-data/executive-orders.csv\")\n",
    "memorandas = pandas.read_csv(\"./data/raw-data/memorandas.csv\")\n",
    "proclamations = pandas.read_csv(\"./data/raw-data/proclamations.csv\")\n",
    "\n",
    "determinations.rename(columns={\"person\": \"Institution\", \"determinations\": \"Text\"}, inplace=True)\n",
    "executive_orders.rename(columns={\"person\": \"Institution\", \"executive-orders\": \"Text\"}, inplace=True)\n",
    "memorandas.rename(columns={\"person\": \"Institution\", \"memorandas\": \"Text\"}, inplace=True)\n",
    "proclamations.rename(columns={\"person\": \"Institution\", \"proclamations\": \"Text\"}, inplace=True)\n",
    "\n",
    "president = pandas.concat([determinations, executive_orders, memorandas, proclamations])\n",
    "president.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "president[\"Text\"] = president[\"Text\"].apply(lambda x: totalClean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_data = pandas.concat([congress, decisions1937, decisions1991, president])\n",
    "institution_data.to_csv(\"./data/clean-data/institution.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \n",
    "              \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', \n",
    "              'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \n",
    "              'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
    "              'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', \n",
    "              'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    "              'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "              'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', \n",
    "              'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "              'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n",
    "              'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', \n",
    "              'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "              's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', \n",
    "              'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "              'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "                'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", \n",
    "                'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lower-case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Vocab class to tokenize a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\" Vocabulary, i.e. structure containing language terms.\n",
    "        This vocabulary can, and should, be abstracted to other\n",
    "        sets of object. For vocabulary, it is words to tokens. \n",
    "        For government institutions, it is institutions to tokens.\n",
    "\n",
    "        Instance attributes:\n",
    "            word2id: dictionary mapping words to indices\n",
    "            unk_id: index for UNK\n",
    "            id2words: dictionary mapping indices to words\n",
    "    \"\"\"\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init Vocab Instance.\n",
    "\n",
    "        :param word2id: dictionary mapping words to indices\n",
    "        :type word2id: dict[str, int]\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id[PAD] = 0  # Pad Token\n",
    "            self.word2id[UNK] = 1  # Unknown Token\n",
    "        self.unk_id = self.word2id[UNK]\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for unk\n",
    "        token if the word is out of vocabulary\n",
    "\n",
    "        :param word: word to look up\n",
    "        :type word: str\n",
    "        :returns: index of word\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "    \n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by Vocab.\n",
    "        \n",
    "        :param word: word to look up\n",
    "        :type word: str\n",
    "        :returns whether word is in vocab\n",
    "        :rtype: bool\n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit Vocab directly.\n",
    "        \"\"\"\n",
    "        raise ValueError(\"Vocab is readonly\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in Vocab.\n",
    "        \n",
    "        :returns: number of words in Vocab\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of Vocab to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return \"Vocabulary[size=%d]\" % len(self)\n",
    "    \n",
    "    def word_from_id(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        \n",
    "        :param wid: word index\n",
    "        :type: int\n",
    "        :returns: word corresponding to index\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "    \n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to Vocab, if it is previously unseen.\n",
    "        \n",
    "        :param word: to add to Vocab\n",
    "        :type word: str\n",
    "        :returns: index that the word has been assigned\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\" Save Vocab to CSV file, indicated by `path`\n",
    "        \n",
    "        :param path: the relative path to the saving file\n",
    "        :type path: str\n",
    "        \"\"\"\n",
    "        with open(path, \"w\") as f:\n",
    "            write = csv.writer(f)\n",
    "            write.writerows([[w, id] for w, id in self.word2id.items()])\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\" Load Vocab from CSV file, indicated by `path`\n",
    "        \n",
    "        :param path: the relative path to the loading file\n",
    "        :type path: str\n",
    "        :returns: Vocab instance produced from CSV file\n",
    "        :rtype: Vocab\n",
    "        \"\"\"\n",
    "        with open(path) as f:\n",
    "            reader = csv.reader(f)\n",
    "            return Vocab({row[0]: int(row[1]) for row in reader})\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, remove_frac=None, freq_cutoff=None):\n",
    "        \"\"\" Given a corpus, construct a Vocab.\n",
    "        \n",
    "        :param corpus: corpus of text produced by read_corpus function\n",
    "        :type corpus: List[str]\n",
    "        :param remove_frac: remove len * remove_frac number of words\n",
    "        :type remove_frac: float\n",
    "        :param freq_cutoff: if word occurs n < frew_cutoff times, drop the word\n",
    "        :type freq_cutoff: int\n",
    "        :returns: Vocab instance produced from provided corpus\n",
    "        :rtype: Vocab\n",
    "        \"\"\"\n",
    "        vocab_entry = Vocab()\n",
    "        word_freq = Counter(chain(corpus))\n",
    "        if freq_cutoff is None:\n",
    "            freq_cutoff = 1\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print(\"number of word types: {}, number of word types w/ frequency >= {}: {}\"\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_words = sorted(valid_words, key=lambda word: word_freq[word], reverse=True)\n",
    "        if remove_frac is not None:\n",
    "            size = len(top_words) - int(remove_frac * len(top_words))\n",
    "            top_words = top_words[:size]\n",
    "            print(f\"number of unqiue words retained with remove_frac={remove_frac}: {len(top_words)}\")\n",
    "        for word in top_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Vocabs for both words and institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word types: 244187, number of word types w/ frequency >= 1: 244187\n",
      "number of word types: 102, number of word types w/ frequency >= 1: 102\n"
     ]
    }
   ],
   "source": [
    "institution_data = pandas.read_csv(\"./data/clean-data/institution.csv\")\n",
    "institution_data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "corpus = \" \".join(institution_data[\"Text\"].to_list()).split(\" \")\n",
    "institutions = institution_data[\"Institution\"].to_list()\n",
    "\n",
    "word_vocab = Vocab.from_corpus(corpus=corpus)\n",
    "gov_vocab = Vocab.from_corpus(corpus=institutions)\n",
    "\n",
    "word_vocab.save(\"./data/govtext/words.csv\")\n",
    "gov_vocab.save(\"./data/govtext/govs.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Vocabs for both words and institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = Vocab.load(\"./data/vocab/words.csv\")\n",
    "gov_vocab = Vocab.load(\"./data/vocab/govs.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenization functions\n",
    "\n",
    "Should look into PyTorch's [tokenizer](https://pytorch.org/text/stable/data_utils.html) functions. Has some pre-build NLP functionality that is most likely implemented with a faster language. Separation of your tokenizer and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_document(document, vocab):\n",
    "    return [vocab[w] for w in document.split(\" \")]\n",
    "\n",
    "def tokenize_documents(documents, vocab):\n",
    "    output = []\n",
    "    for document in documents:\n",
    "        output.append(tokenize_document(document, vocab))\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_data[\"Institution\"] = institution_data[\"Institution\"].apply(lambda x: gov_vocab[x])\n",
    "institution_data[\"Text\"] = institution_data[\"Text\"].apply(lambda x: tokenize_document(x, word_vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidate an institution's corpus into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_corpus = dict()\n",
    "\n",
    "for index, row in institution_data.iterrows():\n",
    "\n",
    "    if row[\"Institution\"] in institution_corpus.keys():\n",
    "        institution_corpus[row[\"Institution\"]] += row[\"Text\"]\n",
    "    else:\n",
    "        institution_corpus[row[\"Institution\"]] = row[\"Text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenized, consolidated institution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/govtext/institution.csv\", 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows([[gov, tokens] for gov, tokens in institution_corpus.items()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined data loader class and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    \"\"\" LanguageDataset is a torch dataset to interact with the Language data.\n",
    "\n",
    "        Dataset (List[ Tuples[ List[ torch.Tensor ], int ] ]): The vectorized dataset with input and expected output values\n",
    "        Dataset is an abstract class representing a dataset:\n",
    "    \"\"\"\n",
    "    def __init__(self, context, gov, target):\n",
    "        \"\"\" Loads in the context, gov, and target as tensors.\n",
    "\n",
    "        :param context: context tokens on both sides\n",
    "        :type context: List[List[int]]\n",
    "        :param gov: government token\n",
    "        :type gov: List[int]\n",
    "        :param target: middle target token\n",
    "        :type target: List[int]\n",
    "        \"\"\"\n",
    "        self.context = torch.tensor(context)\n",
    "        self.gov = torch.tensor(gov)\n",
    "        self.target = torch.tensor(target)\n",
    "        self.len = len(context)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" Number of samples in dataset\n",
    "\n",
    "        :returns: number of samples in dataset\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" The tensor, output for a given index\n",
    "\n",
    "        :param index: index within dataset\n",
    "        :type index: int\n",
    "        :returns: A tuple (x, y, z) where x is the context, y is the govnernment, z is the target\n",
    "        :rtype: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "        \"\"\"\n",
    "        return self.context[index], self.gov[index], self.target[index]\n",
    "\n",
    "\n",
    "def get_data_loaders(preprocessed_data, batch_size=1, shuffle=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dataset = LanguageDataset(preprocessed_data[\"context_tokens\"], preprocessed_data[\"gov_tokens\"], preprocessed_data[\"target_tokens\"])\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect 1 millions samples for each government institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = dict()\n",
    "preprocessed_data[\"context_tokens\"] = []\n",
    "preprocessed_data[\"gov_tokens\"] = []\n",
    "preprocessed_data[\"target_tokens\"] = []\n",
    "\n",
    "\n",
    "for gov, text in tqdm(institution_corpus.items()):\n",
    "\n",
    "    count = 0\n",
    "    length = len(text)\n",
    "\n",
    "    # Collect 500,000 samples for each Institution\n",
    "    while(count < 500_000):\n",
    "\n",
    "        # Sample randomly from the total corpus\n",
    "        position = math.ceil(random.random() * length)\n",
    "        if position < (2*WINDOW_SIZE + 1):\n",
    "            position = 2*WINDOW_SIZE + 1\n",
    "\n",
    "        context = text[position - (2*WINDOW_SIZE + 1): position - (WINDOW_SIZE + 1)] + text[position - WINDOW_SIZE: position]\n",
    "        if len(context) != 2 * WINDOW_SIZE:\n",
    "            continue\n",
    "        target = text[position - (WINDOW_SIZE + 1)]\n",
    "\n",
    "        preprocessed_data[\"context_tokens\"].append(context)\n",
    "        preprocessed_data[\"gov_tokens\"].append(gov)\n",
    "        preprocessed_data[\"target_tokens\"].append(target)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "\n",
    "pandas.DataFrame(data=preprocessed_data).to_csv(f\"./data/input/inputRandom_window{WINDOW_SIZE}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all samples (~100 million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pandas.read_csv(filepath_or_buffer=f\"./data/input/inputRandom_window{WINDOW_SIZE}.csv\", \n",
    "                                    converters={\"context_tokens\": lambda x: [int(y) for y in x.strip('][').split(', ')]}\n",
    "                                    )\n",
    "\n",
    "loader = get_data_loaders(preprocessed_data=preprocessed_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 100,000 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pandas.read_csv(filepath_or_buffer=f\"./data/input/inputRandom_window{WINDOW_SIZE}.csv\")\n",
    "\n",
    "indices = set()\n",
    "for i in range(100_000):\n",
    "    index = random.randrange(0, len(preprocessed_data))\n",
    "    indices.add(index)\n",
    "\n",
    "preprocessed_data = preprocessed_data.iloc[list(indices)]\n",
    "\n",
    "preprocessed_data[\"context_tokens\"] = preprocessed_data[\"context_tokens\"].apply(lambda x: [int(y) for y in x.strip('][').split(', ')])\n",
    "loader = get_data_loaders(preprocessed_data=preprocessed_data.to_dict(\"list\"), batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/inejc/paragraph-vectors/blob/master/paragraphvec/models.py\n",
    "# Implemention of object embedding with target word predictions\n",
    "\n",
    "# https://github.com/inejc/paragraph-vectors/tree/master\n",
    "\n",
    "# https://github.com/OlgaChernytska/word2vec-pytorch/tree/main\n",
    "\n",
    "# https://github.com/jeffchy/pytorch-word-embedding/blob/master/CBOW.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and Set PyTorch backend device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Verify mps support (Apple Silicon)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "device = \"\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Gov2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gov2Vec_Model(nn.Module):\n",
    "    \"\"\" Gov2Vec Model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, gov_size, word_embedding_dim, gov_embedding_dim):\n",
    "\n",
    "        super(Gov2Vec_Model, self).__init__()\n",
    "        # Weights should be given to CrossEntropyLoss that incorporate the frequency of words\n",
    "        # in the dataset. Weights for the minority classes (words) should be higher.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.gov_size = gov_size\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.gov_embedding_dim = gov_embedding_dim\n",
    "\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=word_embedding_dim\n",
    "        )\n",
    "        self.gov_embedding = nn.Embedding(\n",
    "            num_embeddings=gov_size,\n",
    "            embedding_dim=gov_embedding_dim\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=word_embedding_dim+gov_embedding_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, context, gov):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        context_embedding = self.word_embedding(context)\n",
    "        gov_embedding = self.gov_embedding(gov)\n",
    "\n",
    "        # Can add, or mean over context tokens\n",
    "        # Can concat or add or mean context tokens with gov token\n",
    "        context_embedding = context_embedding.mean(axis=1)\n",
    "        combined = torch.cat((gov_embedding, context_embedding), 1)\n",
    "        out = self.linear(combined)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def load_model(self, save_path, is_state_dict=False):\n",
    "        if not is_state_dict:\n",
    "            saved_model = torch.load(save_path)\n",
    "            self.load_state_dict(saved_model.state_dict())\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    def save_model(self, save_path, is_state_dict=False):\n",
    "        if is_state_dict:\n",
    "            torch.save(self.state_dict(), save_path)\n",
    "        else:\n",
    "            torch.save(self, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for (context_batch, gov_batch, target_batch) in tqdm(train_loader, leave=False, desc=\"Training batches\"):\n",
    "        optimizer.zero_grad()\n",
    "        batch += 1\n",
    "        output = model(context_batch.to(device), gov_batch.to(device)).to(device)\n",
    "        loss = model.criterion(output, target_batch.to(device))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Loss: \" + str(total_loss / batch))\n",
    "    return total_loss / batch\n",
    "\n",
    "def evaluation(model, val_loader, optimizer):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for (context_batch, gov_batch, target_batch) in tqdm(val_loader, leave=False, desc=\"Validation Batches\"):\n",
    "        outputs = model(context_batch.to(device), gov_batch.to(device)).to(device)\n",
    "        loss += model.criterion(outputs, target_batch.to(device))\n",
    "    loss /= len(val_loader)\n",
    "    print(\"Validation Loss: \" + str(loss.item()))\n",
    "    # print(\"Validation Accuracy: \" + str(correct))\n",
    "    print()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_and_evaluate(number_of_epochs, model, train_loader, val_loader=None, min_loss=0, learning_rate=0.001):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    loss_values = [[],[]]\n",
    "\n",
    "    for epoch in trange(number_of_epochs, desc=\"Epochs\"):\n",
    "        curr_loss = train_epoch(model, train_loader, optimizer)\n",
    "        loss_values[0].append(curr_loss)\n",
    "        # if val_loader is not None:\n",
    "        #    curr_loss_val = evaluation(model, val_loader, optimizer)\n",
    "        #    loss_values[1].append(curr_loss_val)\n",
    "        if curr_loss <= min_loss: return loss_values\n",
    "\n",
    "    return loss_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Gov2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gov2Vec_Model(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (word_embedding): Embedding(244189, 150)\n",
       "  (gov_embedding): Embedding(104, 150)\n",
       "  (linear): Linear(in_features=300, out_features=244189, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Gov2Vec_Model(\n",
    "    vocab_size=len(word_vocab), \n",
    "    gov_size=len(gov_vocab),\n",
    "    word_embedding_dim=150,\n",
    "    gov_embedding_dim=150\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Gov2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a676f6701c47a18888349722bb8808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997f677b38874abfa98b1b0f3e2b8078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches:   0%|          | 0/3091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.477017898923639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c5da2afc33435db018bd03da4a0711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches:   0%|          | 0/3091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.80784002718745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497f98cbe6ba4f27a160215beadd6b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches:   0%|          | 0/3091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.8650567180629842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a900b46d754ceea6e3b5b177ea87fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches:   0%|          | 0/3091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.3638254211620775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893654bba9424363aaa9a3f9075e1fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches:   0%|          | 0/3091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.1140751017987323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[6.477017898923639,\n",
       "  4.80784002718745,\n",
       "  3.8650567180629842,\n",
       "  3.3638254211620775,\n",
       "  3.1140751017987323],\n",
       " []]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    number_of_epochs=5,\n",
    "    model=model,\n",
    "    train_loader=loader,\n",
    "    min_loss=0.2,\n",
    "    learning_rate=0.015\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./data/model/e5b16lr025s10000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Court v Court tensor(0.0835, device='mps:0', grad_fn=<SumBackward1>)\n",
      "President v Court tensor(0.0675, device='mps:0', grad_fn=<SumBackward1>)\n",
      "Court v 'truth' tensor(0.0633, device='mps:0', grad_fn=<SumBackward1>)\n",
      "President v 'truth' tensor(-0.0306, device='mps:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "gov_token1 = int(gov_vocab[\"Robert Court\"])\n",
    "gov_token2 = int(gov_vocab[\"Rehnquist Court\"])\n",
    "gov_token3 = int(gov_vocab[\"Joseph R. Biden\"])\n",
    "\n",
    "word_token1 = int(word_vocab[\"truth\"])\n",
    "\n",
    "gov_embed1 = model.gov_embedding(torch.tensor(gov_token1, device=device))\n",
    "gov_embed2 = model.gov_embedding(torch.tensor(gov_token2, device=device))\n",
    "gov_embed3 = model.gov_embedding(torch.tensor(gov_token3, device=device))\n",
    "\n",
    "word_embed1 = model.word_embedding(torch.tensor(word_token1, device=device))\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "print(\"Court v Court\", cos(gov_embed1, gov_embed2))\n",
    "print(\"President v Court\", cos(gov_embed3, gov_embed2))\n",
    "\n",
    "print(\"Court v 'truth'\", cos(gov_embed1, word_embed1))\n",
    "print(\"President v 'truth'\", cos(gov_embed3, word_embed1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(cVocab, cGov, qVocab, query, model):\n",
    "    \"\"\" Gives the token with the maximum cosine similarity\n",
    "    score from the comparison vocab against the query\n",
    "    \n",
    "    :param cVocab: vocab to compare the query to\n",
    "    :type cVocab: Vocab\n",
    "    :param cGov: whether the comparison vocab is a government\n",
    "    :type cGov: bool\n",
    "    :param qVocab: vocab of the query\n",
    "    :type qVocab: Vocab\n",
    "    :param query: query of words to compare, each word in the query can be positive or negative\n",
    "    :type query: List[Tuple(str, int)]\n",
    "    \"\"\"\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    if cGov:\n",
    "        cEmbedding = model.gov_embedding\n",
    "    else:\n",
    "        cEmbedding = model.word_embedding\n",
    "\n",
    "    qEmbedding = model.word_embedding\n",
    "\n",
    "    query_len = len(query)\n",
    "    query_vec = sum(\n",
    "        map(\n",
    "            lambda x: qEmbedding(torch.tensor(qVocab[x[0]], device=device)) * x[1],\n",
    "            query\n",
    "        )\n",
    "    ) / query_len\n",
    "\n",
    "    max = (\"\", -10000)\n",
    "    for word, id in cVocab.word2id.items():\n",
    "\n",
    "        # Exclude comparison of words in the query\n",
    "        if word in map(lambda x: x[0],query):\n",
    "            continue\n",
    "\n",
    "        score = cos(cEmbedding(torch.tensor(id, device=device)), query_vec)\n",
    "        if score > max[1]:\n",
    "            max = (word, score)\n",
    "\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Senate 116th', tensor(0.2217, device='mps:0', grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(gov_vocab, True, word_vocab, [(\"validity\", 1), (\"truth\", 1)], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Senate 117th', tensor(0.2138, device='mps:0', grad_fn=<SumBackward1>))\n",
      "('Senate 104th', tensor(0.2097, device='mps:0', grad_fn=<SumBackward1>))\n",
      "('House 103th', tensor(0.1977, device='mps:0', grad_fn=<SumBackward1>))\n",
      "('Senate 116th', tensor(0.2217, device='mps:0', grad_fn=<SumBackward1>))\n",
      "('Senate 111th', tensor(0.2222, device='mps:0', grad_fn=<SumBackward1>))\n",
      "('Senate 96th', tensor(0.1679, device='mps:0', grad_fn=<SumBackward1>))\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    [(\"candidate\",1),(\"elected\",1),(\"campaign\",1)],\n",
    "    [(\"long\",1),(\"term\",1),(\"government\",1),(\"carerr\",1)],\n",
    "    [(\"rule\",1),(\"precedent\",1),(\"interpret\",1)],\n",
    "    [(\"validity\",1), (\"truth\",1)],\n",
    "    [(\"statistics\",1),(\"science\",1),(\"data\",1),(\"story\",-1),(\"anecdote\",-1)],\n",
    "    [(\"order\",1),(\"direct\",1),(\"contemplate\",-1),(\"consider\",1)]\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = similarity(gov_vocab, True, word_vocab, query, model)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\"113th House Economic\": [\n",
    "        (\"climate\", 1, \"word\"),\n",
    "        (\"emissions\", 1, \"word\"),\n",
    "        (\"House 113th\", 1, \"gov\"),\n",
    "        (\"Barack Obama\", -1, \"gov\"),\n",
    "        (\"economy\", 1, \"word\"),\n",
    "        (\"environment\", -1, \"word\"),\n",
    "    ]},\n",
    "    {\"113th House Environmental\": [\n",
    "        (\"climate\", 1, \"word\"),\n",
    "        (\"emissions\", 1, \"word\"),\n",
    "        (\"House 113th\", 1, \"gov\"),\n",
    "        (\"Barack Obama\", -1, \"gov\"),\n",
    "        (\"economy\", -1, \"word\"),\n",
    "        (\"environment\", 1, \"word\"),\n",
    "    ]},\n",
    "    {\"Obama Economic\": [\n",
    "       (\"climate\", 1, \"word\"),\n",
    "        (\"emissions\", 1, \"word\"),\n",
    "        (\"House 113th\", -1, \"gov\"),\n",
    "        (\"Barack Obama\", 1, \"gov\"),\n",
    "        (\"economy\", 1, \"word\"),\n",
    "        (\"environment\", -1, \"word\"),\n",
    "    ]},\n",
    "    {\"Obama Environmental\": [\n",
    "        (\"climate\", 1, \"word\"),\n",
    "        (\"emissions\", 1, \"word\"),\n",
    "        (\"House 113th\", -1, \"gov\"),\n",
    "        (\"Barack Obama\", 1, \"gov\"),\n",
    "        (\"economy\", -1, \"word\"),\n",
    "        (\"environment\", 1, \"word\"),\n",
    "    ]},\n",
    "    {\"106th House Oil\": [\n",
    "        (\"war\", 1, \"word\"),\n",
    "        (\"House 106th\", 1, \"gov\"),\n",
    "        (\"House 107th\", -1, \"gov\"),\n",
    "        (\"oil\", 1, \"word\"),\n",
    "        (\"terrorism\", -1, \"word\"),\n",
    "    ]},\n",
    "    {\"106th House Terrorism\": [\n",
    "        (\"war\", 1, \"word\"),\n",
    "        (\"House 106th\", 1, \"gov\"),\n",
    "        (\"House 107th\", -1, \"gov\"),\n",
    "        (\"oil\", -1, \"word\"),\n",
    "        (\"terrorism\", 1, \"word\"),\n",
    "    ]},\n",
    "    {\"107th House Oil\": [\n",
    "        (\"war\", 1, \"word\"),\n",
    "        (\"House 106th\", -1, \"gov\"),\n",
    "        (\"House 107th\", 1, \"gov\"),\n",
    "        (\"oil\", 1, \"word\"),\n",
    "        (\"terrorism\", -1, \"word\"),\n",
    "    ]},\n",
    "    {\"107th House Terrorism\": [\n",
    "        (\"war\", 1, \"word\"),\n",
    "        (\"House 106th\", -1, \"gov\"),\n",
    "        (\"House 107th\", 1, \"gov\"),\n",
    "        (\"oil\", -1, \"word\"),\n",
    "        (\"terrorism\", 1, \"word\"),\n",
    "    ]},\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
